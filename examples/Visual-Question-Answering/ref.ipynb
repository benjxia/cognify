{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Question Answering\n",
    "The task of VQA for llm means it needs to answer a question based on an image. \n",
    "\n",
    "Input = [question,image]\n",
    "\n",
    "Output = [answer (ground truth answer is called annottation)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before running the repo\n",
    "Please download the sub-sampled VQA dataset from here: https://shorturl.at/VzNaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['OPENAI_API_KEY'] = ...\n",
    "os.environ['COGNIFY_TELEMETRY'] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cognify\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "DEFAULT_NR_SAMPLES = 50\n",
    "\n",
    "@cognify.register_data_loader\n",
    "def load_data(path_to_data, nr_samples=DEFAULT_NR_SAMPLES):\n",
    "    ''' \n",
    "    Should return a list [train_data, eval_data, test_data]\n",
    "    Each train_data, eval_data, test_data should be in form\n",
    "    data = (input, label). Which in our case will be\n",
    "    \n",
    "    label should be the ground truth annottation\n",
    "    input should be dictionaity that annottated workflow() func can handle\n",
    "    \n",
    "    '''\n",
    "    TRAIN_SPLIT, EVAL_SPLIT, TEST_SPLIT = 0.8, 0.1, 0.1\n",
    "    assert TRAIN_SPLIT + EVAL_SPLIT + TEST_SPLIT == 1.0, 'Splits should sum to 1.0'\n",
    "\n",
    "    ANNOTTATION_FILE_PATH = os.path.join(path_to_data, 'v2_mscoco_val2014_annotations_subsampled.json')\n",
    "    QUESTIONS_FILE_PATH = os.path.join(path_to_data, 'v2_OpenEnded_mscoco_val2014_questions_subsampled.json')\n",
    "    IMAGE_FOLDER = os.path.join(path_to_data, 'val2014') # image in form \"COCO_val2014_000000000827.jpg\"\n",
    "\n",
    "    def find_question_for_img_id(image_id: int, questions: list[dict]):\n",
    "        #TODO: use a dict for faster lookup\n",
    "        for question in questions:\n",
    "            if question['image_id'] == image_id:\n",
    "                return question\n",
    "        # raise ValueError, indicating the sub-sampling was not done correctly\n",
    "        raise ValueError('No question found for image_id: {}'.format(image_id))\n",
    "    \n",
    "    def find_annotation_forquestion_id(image_id, question_id, annotations):\n",
    "        #TODO: use a dict for faster lookup\n",
    "        for annotation in annottations:\n",
    "            if annotation['image_id'] == image_id and annotation['question_id'] == question_id:\n",
    "                return annotation\n",
    "        # raise ValueError, indicating the sub-sampling was not done correctly\n",
    "        raise ValueError('No annotation found for image_id: {} and question_id: {}'.format(image_id, question_id))\n",
    "    \n",
    "    def parse_annotation(annottation: dict):\n",
    "        ''' \n",
    "        from {'image_id': 0, 'question_id': 0, 'answer_type': '...', 'answers': ['...']} to {'answer': '...'} \n",
    "        there are multiple answers, all similar to each other. But since we'll be using llm_as_judge, \n",
    "        it'll account for similartiy by itself.\n",
    "\n",
    "        #TODO: Think about handling multiple answers\n",
    "        '''\n",
    "        return annottation['answers'][0][\"answer\"]\n",
    "\n",
    "\n",
    "    all_data = [] # in form tuple(path_to_image, question, annotation)\n",
    "    annottations = json.load(open(ANNOTTATION_FILE_PATH, 'r'))\n",
    "    questions = json.load(open(QUESTIONS_FILE_PATH, 'r'))\n",
    "    images = os.listdir(IMAGE_FOLDER)\n",
    "\n",
    "    # pair each image with a question and an annotation\n",
    "    # randomize images\n",
    "    random.shuffle(images) \n",
    "    for i in range(nr_samples):\n",
    "        image_path = images[i]\n",
    "        image_id = int(image_path.split('_')[-1].split('.')[0])\n",
    "\n",
    "        # question in form {'image_id': 0, 'question': '...'}\n",
    "        question = find_question_for_img_id(image_id, questions) \n",
    "        # annotation in form {'image_id': 0, 'question_id': 0, 'answer_type': '...', 'answers': ['...']}\n",
    "        annotation = find_annotation_forquestion_id(image_id, question['question_id'], annottations) \n",
    "\n",
    "        # populate tuple and append to data\n",
    "        question = question['question']; annotation = parse_annotation(annotation)\n",
    "        all_data.append((os.path.join(IMAGE_FOLDER, image_path), question, annotation))\n",
    "    \n",
    "    # transform all data into [tuple(input, label)] \n",
    "    # where input = {'query': 'query, img_path: 'path_to_image'} and label = {'ground_truth': 'annotation'}\n",
    "    all_data = [( \n",
    "        # workflow input\n",
    "        {'query': question, \n",
    "        'img_path': image_path}, \n",
    "\n",
    "        # ground truth\n",
    "        {'ground_truth': annotation}) \n",
    "        for image_path, question, annotation in all_data]\n",
    "    print(\"len(all_data): \", len(all_data))\n",
    "\n",
    "    idx_train, idx_eval, idx_test = len(all_data)*TRAIN_SPLIT, len(all_data)*(TRAIN_SPLIT+EVAL_SPLIT), len(all_data)\n",
    "    idx_train, idx_eval, idx_test = int(idx_train), int(idx_eval), int(idx_test)\n",
    "    train_data, eval_data, test_data = all_data[:idx_train], all_data[idx_train:idx_eval], all_data[idx_eval:idx_test]\n",
    "    return [train_data, eval_data, test_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cognify\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize the model\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Force agent to respond with a score\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "class Assessment(BaseModel):\n",
    "    score: int\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=Assessment)\n",
    "\n",
    "@cognify.register_evaluator\n",
    "def llm_judge(workflow_input, workflow_output, ground_truth):\n",
    "    evaluator_prompt = \"\"\"\n",
    "You are a dictionary evaluator. Your task is to evaluate whether two sentences describe the same thing.\n",
    "Please rate the answer with a score between 0 to 2. Where 0 is completely wrong, 1 is somewhat similar, 2 is very similar.\n",
    "    \"\"\"\n",
    "    evaluator_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", evaluator_prompt),\n",
    "            (\"human\", \"sentence 1:\\n{problem}\\n\\nsentence 2:\\n{solution}\\n\\n\"),\n",
    "        ]\n",
    "    )\n",
    "    evaluator_agent = evaluator_template | model | parser\n",
    "    assess = evaluator_agent.invoke(\n",
    "        {\n",
    "            \"sentence_1\": workflow_output,\n",
    "            \"sentence_2\": ground_truth, \n",
    "        }\n",
    "    )\n",
    "    return assess.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognify.hub.search import default\n",
    "\n",
    "model_configs = [\n",
    "    # OpenAI models\n",
    "    cognify.LMConfig(model='gpt-4o-mini', kwargs={'temperature': 0, 'max_tokens': 300}),\n",
    "    cognify.LMConfig(model='gpt-4o', kwargs={'temperature': 0, 'max_tokens': 300}),\n",
    "]\n",
    "\n",
    "search_settings = default.create_search(\n",
    "    model_selection_cog=model_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the search (cognify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, dev = load_data('VQA-sub-sampled', nr_samples=50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt_cost, pareto_frontier, opt_logs = cognify.optimize(\n",
    "        script_path=\"workflow.py\",\n",
    "        control_param=search_settings,\n",
    "        train_set=train,\n",
    "        val_set=val,\n",
    "        eval_fn=llm_judge,\n",
    "        force=True, # This will overwrite the existing results\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
