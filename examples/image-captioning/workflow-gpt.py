<<<<<<< HEAD
import base64
import dotenv
import cognify
import logging
import os

# Set logging level
logging.basicConfig(level=logging.INFO)
dotenv.load_dotenv()

##### ----- Helper Functions ----- #####
def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

##### ----- Define Agent Configuration ----- #####
CAPTION_PROMPT = """
You are an image understanding expert. Your task is to analyze an image and describe its contents clearly.
Identify key objects, actions, and any relevant context. Provide a detailed and accurate caption for the image.
"""

# Configure LLM settings
caption_lm_config = cognify.LMConfig(
    custom_llm_provider='openai',
    model='gpt-4o-mini',
    kwargs={'temperature': 0.0}
)

# Create Caption agent
caption_agent = cognify.Model(
    agent_name='image_caption',
    system_prompt=CAPTION_PROMPT,
    input_variables=[
        cognify.Input(name='image', image_type='png')
    ],
    output=cognify.OutputLabel(name='caption'),
    lm_config=caption_lm_config
)

##### ----- Agent Function ----- #####
def generate_caption(image_path):
    information = {
        'image': encode_image(image_path)  # base64 encoded image
    }
    caption = caption_agent(inputs=information)
    return caption

##### ----- Workflow Definition ----- #####
@cognify.register_workflow
def vlm_workflow(workflow_input):
    caption = generate_caption(workflow_input)
    return {
        "workflow_output": caption
=======
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import dotenv
import cognify
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

dotenv.load_dotenv()
model = ChatOpenAI(model="gpt-4o", temperature=0, max_tokens=300)

# BLIP model for vision-language tasks
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
vlm_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

image_analysis_prompt = """
You are an image understanding expert. Your task is to analyze an image and describe its contents clearly.
Identify key objects, actions, and any relevant context.
"""

image_analysis_template = ChatPromptTemplate.from_messages(
    [
        ("system", image_analysis_prompt),
        ("human", "image description:\n{caption}\n"),
    ]
)

image_analysis_agent = image_analysis_template | model

classification_prompt = """
You are an image classifier. Given a description of an image, your task is to categorize it into one of the predefined classes.
"""

classification_template = ChatPromptTemplate.from_messages(
    [
        ("system", classification_prompt),
        ("human", "image description:\n{caption}\n"),
    ]
)

classification_agent = classification_template | model

def generate_caption(image_path):
    image = Image.open(image_path).convert("RGB")
    inputs = processor(image, return_tensors="pt")
    output = vlm_model.generate(**inputs)
    caption = processor.decode(output[0], skip_special_tokens=True)
    return caption

@cognify.register_workflow
def vlm_workflow(workflow_input):
    caption = generate_caption(workflow_input)
    # analyzed_caption = image_analysis_agent.invoke({"caption": caption}).content
    # classification = classification_agent.invoke({"caption": analyzed_caption}).content
    return {
        "workflow_output": caption,              # Generated by BLIP
        # "workflow_output": analyzed_caption,    # Generated by GPT
        # "classification": classification # GPT's classification
>>>>>>> 0db8c22c37fa725b55cd7816ee3f8e71d3a9317e
    }

if __name__ == "__main__":
    image_file = "test_image.jpg"  # TODO: replace with img folder
    output = vlm_workflow(image_file)
    print(output)